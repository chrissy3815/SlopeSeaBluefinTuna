---
title: "SlopeSeaOtolithAnalyses_2020Reads"
author: "Chrissy Hernandez"
date: "7/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xlsx)
```

First, we need to shuffle the read order for read 2:
```{r}
read1<- read.xlsx('data/SlopeSeaOto2016_FullRead_1.xlsx', sheetIndex = 1)
filenames<- read1$Fish
shuffledorder<- sample(filenames)
write.csv(shuffledorder, file='data/SlopeSeaOto_2020Reads_read2order.csv')
```

Next, some QC checks:
```{r}
# bring in the data from otoliths read 1:
read1<- read.xlsx('data/SlopeSeaOto2016_FullRead_1.xlsx', sheetIndex = 1)
# bring in the data from otoliths read 2:
read2<- read.xlsx('data/SlopeSeaOto2016_FullRead_2_retakes.xlsx', sheetIndex = 1)
names(read2)[length(names(read2))]<- "Age2"

# use merge to combine both reads:
bothreads<- merge(read1[,c('Fish', 'Age')], read2[,c('Fish', 'Age2')], by='Fish', all.x=F)
# difference between the two reads:
bothreads$diff = abs(bothreads$Age-bothreads$Age2)
summary(bothreads$diff)
# print the ones that differ by more than 1 day:
bothreads[bothreads$diff>1,c("Fish", "diff")]
# shuffle the order:
shuffledorder2<- sample(bothreads$Fish[bothreads$diff>1])
write.csv(shuffledorder2, file='data/SlopeSeaOto_2020Reads_read3order.csv')
```

Check read 3:
```{r}
# bring in the read3 data:
read3<- read.xlsx('data/SlopeSeaOto2016_FullRead_3.xlsx', sheetIndex = 1)
names(read3)[length(names(read3))]<- "Age3"
all3reads<- merge(bothreads, read3[,c("Fish", "Age3")])
all3reads$diff1<- abs(all3reads$Age-all3reads$Age3)
all3reads$diff2<- abs(all3reads$Age2-all3reads$Age3)
all3reads

```

Okay, now we can treat this data as final and clean it up for use in figures, etc:
```{r}
# If Read 1 and Read 2 agree within 1 day, use Read 2
read2_tokeep<- bothreads[bothreads$diff<=1,"Fish"]
otolith_data_clean<- read2[read2$Fish %in% read2_tokeep,]
names(otolith_data_clean)[length(names(otolith_data_clean))]<- "Age"
# If Read 3 agrees with either Read 1 or Read 2 to within 1 day, use Read 3:
read3_tokeep<- all3reads[all3reads$diff1<=1 | all3reads$diff2<=1,"Fish"]
names(read3)[length(names(read3))]<- "Age"
otolith_data_clean<- rbind(otolith_data_clean, read3[read3$Fish %in% read3_tokeep,])

# Let's also get rid of the empty columns:
NAcolumns<- apply(otolith_data_clean, MARGIN=2, function(x){sum(is.na(x))})
I<- which(NAcolumns==54)
otolith_data_clean<- otolith_data_clean[,-I]

# break up the "Fish" column to get cruise, station, gear, fish:
fishID<- strsplit(otolith_data_clean$Fish, "-")
# get out the cruise IDs:
cruise<- sapply(fishID, '[', 1)
head(cruise)
otolith_data_clean$Cruise<- sapply(strsplit(cruise, "Stn"), '[', 1)
# get out the station numbers:
station<- sapply(strsplit(otolith_data_clean$Fish, "Stn"), '[', 2)
otolith_data_clean$Station<- as.numeric(sapply(strsplit(station, "-"), '[', 1))
# get the gear IDs:
otolith_data_clean$Gear<- sapply(strsplit(station, "-"), '[', 2)
# get the fish numbers:
fishNum<- sapply(strsplit(station, "-"), '[', 3)
otolith_data_clean$FishNum<- sapply(strsplit(fishNum, '.tif'), '[', 1)
head(otolith_data_clean)

## Adding other metadata from the tow:
slopeseaoperations<- read.xlsx('data/SlopeSeaOperations.xlsx', sheetIndex = 2)
metadata<- slopeseaoperations[,c("cruiseid", "siteid", "event.time", "deployment", "lat", "lon", "max.ctd.depth", "gm_1", "tot_1", "gm_2", "tot_2")]
names(metadata)<- c("Cruise", "Station", "DateTime", "GearType", "lat", "lon", "MaxDepth", "Bongo1", "VolumeFiltered_B1", "Bongo2", "VolumeFiltered_B2")
metadata$LatDec<- floor(metadata$lat/100)+(metadata$lat-floor(metadata$lat/100)*100)/60
metadata$LonDec<- floor(metadata$lon/100)+(metadata$lon-floor(metadata$lon/100)*100)/60
head(metadata)



```
Data to give to Irina for backtracking:

```{r}
# first, take the ones that have ages:
for_backtracking<- otolith_data_clean[,c("Cruise", "Station", "Gear", "FishNum", "Age")]
head(for_backtracking)

## Add latitude and longitude of collection location, and date/time of collection
# First for the 2N3 samples:
I<- which(otolith_data_clean$Gear=="2N3")
query_stations<- unique(otolith_data_clean[I,c("Cruise", "Station")])
J<- which(metadata$GearType=="CTD/IKMT Oblique")
framenet_samples<- merge(query_stations, metadata[J, c("Cruise", "Station", "LatDec", "LonDec", "DateTime")])
framenet_samples$Gear<- "2N3"
# Next, for the baby Bongo samples:
I<- which(for_backtracking$Gear=="2B1")
query_stations<- unique(otolith_data_clean[I,c("Cruise", "Station")])
J<- which(metadata$GearType=="CTD/Bongo Oblique")
babybongo_samples<- merge(query_stations, metadata[J, c("Cruise", "Station", "LatDec", "LonDec", "DateTime")])
babybongo_samples$Gear<- "2B1"
# Next, for the 6B3I samples:
I<- which(for_backtracking$Gear=="6B3I")
query_stations<- unique(otolith_data_clean[I,c("Cruise", "Station")])
J<- which(metadata$GearType=="CTD/Bongo Oblique")
bongoI_samples<- merge(query_stations, metadata[J, c("Cruise", "Station", "LatDec", "LonDec", "DateTime")])
bongoI_samples$Gear<- "6B3I"
# Next, for the 6B3Z samples:
I<- which(for_backtracking$Gear=="6B3Z")
query_stations<- unique(otolith_data_clean[I,c("Cruise", "Station")])
J<- which(metadata$GearType=="CTD/Bongo Oblique")
bongoZ_samples<- merge(query_stations, metadata[J, c("Cruise", "Station", "LatDec", "LonDec", "DateTime")])
bongoZ_samples$Gear<- "6B3Z"
# paste all these rows together
to_merge<- rbind(framenet_samples, babybongo_samples, bongoI_samples, bongoZ_samples)

# merge metadata to ages:
for_backtracking<- merge(for_backtracking, to_merge)

# Correct otolith "ages" to days post hatch: need to subtract 1 because I mark the edge of the core, and then add 2 for days post hatch (Malca et al. 2017 references Yufera et al 2014)
for_backtracking$Age<- for_backtracking$Age+1
# write the csv out:
write.csv(for_backtracking, file='results/SlopeSea2016_agedlarvae_forbacktracking.csv')

#### Estimating ages for the rest of the larvae that have lengths:


```

